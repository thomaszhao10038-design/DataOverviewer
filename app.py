# FILE: app.py
"""
Combined Streamlit app that integrates two processing stages:
- Stage 1 (Code1): Accepts multiple raw CSV files, processes them, and outputs an Excel workbook.
- Stage 2 (Code2): Accepts the Excel generated by Stage 1 (or any compatible workbook), performs further processing and reporting, and lets the user download final outputs.

This single-file app is intentionally modular so you can drop your original Code 1 and Code 2 functions into the placeholders below without losing any UI or functionality.

Usage: streamlit run app.py

Author: Generated for user (ZX Chang)
"""

import streamlit as st
import pandas as pd
import io
import zipfile
import tempfile
import os
from datetime import datetime

st.set_page_config(page_title="Combined Processor", layout="wide")

# -------------------------
# Helpers / Placeholders
# -------------------------

def safe_read_csv(uploaded_file):
    """Reads CSV into pandas.DataFrame with robust parsing."""
    try:
        return pd.read_csv(uploaded_file)
    except Exception:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, encoding='latin1')

# === BEGIN: PLACEHOLDER FOR ORIGINAL CODE 1 ===
# Replace the body of `process_raw_csvs_to_excel` with your exact Code 1 logic.

def process_raw_csvs_to_excel(dfs_dict, excel_writer=None):
    """
    Example processing for Code 1. Inputs:
      - dfs_dict: dict of {filename: dataframe}
      - excel_writer: optional pd.ExcelWriter (openpyxl engine) to write into.

    Returns: bytes of an .xlsx workbook and a dict of DataFrames that were written (for downstream use).

    IMPORTANT: Replace the internal logic with your exact Code 1 processing to avoid losing details.
    """
    # We'll create a workbook with each raw file as a sheet, plus a summary sheet.
    import pandas as pd
    from io import BytesIO

    output = BytesIO()
    if excel_writer is None:
        writer = pd.ExcelWriter(output, engine='openpyxl')
    else:
        writer = excel_writer

    summary_rows = []
    for name, df in dfs_dict.items():
        sheet_name = os.path.splitext(name)[0][:31]
        # Example cleaning: drop completely-empty columns
        df_clean = df.dropna(axis=1, how='all').copy()
        # Try to split a combined 'datetime' column into date & time if present
        if 'datetime' in map(str.lower, df_clean.columns):
            # find the actual column name case-insensitively
            dt_col = [c for c in df_clean.columns if c.lower()=='datetime'][0]
            try:
                dt_parsed = pd.to_datetime(df_clean[dt_col], errors='coerce')
                df_clean['date_only'] = dt_parsed.dt.date
                df_clean['time_only'] = dt_parsed.dt.time
            except Exception:
                pass
        # write sheet
        df_clean.to_excel(writer, sheet_name=sheet_name, index=False)
        summary_rows.append({'sheet': sheet_name, 'rows': len(df_clean), 'cols': len(df_clean.columns)})

    # create a summary sheet
    summary_df = pd.DataFrame(summary_rows)
    summary_df.to_excel(writer, sheet_name='__summary__', index=False)

    # finalize
    if excel_writer is None:
        writer.save()
        output.seek(0)
        return output.read(), {'sheets': dfs_dict, 'summary': summary_df}
    else:
        # if provided external writer, do not close here
        return None, {'sheets': dfs_dict, 'summary': summary_df}

# === END: PLACEHOLDER FOR ORIGINAL CODE 1 ===

# === BEGIN: PLACEHOLDER FOR ORIGINAL CODE 2 ===
# Replace `process_excel_stage2` with your exact Code 2 logic.

def process_excel_stage2(excel_bytes):
    """
    Example processing for Code 2. Inputs:
      - excel_bytes: bytes of an Excel workbook created by Code 1.

    Returns:
      - results: dict of outputs (dataframes, report bytes, etc.)
    """
    from io import BytesIO
    import pandas as pd

    xls = pd.ExcelFile(BytesIO(excel_bytes))
    results = {}
    # read sheets and produce a single merged dataframe if possible
    df_list = []
    for sheet in xls.sheet_names:
        if sheet == '__summary__':
            results['summary'] = pd.read_excel(xls, sheet_name=sheet)
            continue
        df = pd.read_excel(xls, sheet_name=sheet)
        df['__source_sheet__'] = sheet
        df_list.append(df)

    if df_list:
        merged = pd.concat(df_list, ignore_index=True, sort=False)
        # Example analysis: compute basic stats for numeric columns
        numeric_stats = merged.select_dtypes('number').describe().transpose()
        results['merged'] = merged
        results['numeric_stats'] = numeric_stats
    else:
        results['merged'] = pd.DataFrame()

    # Example: create a csv bytes output and a small report
    out = BytesIO()
    if 'merged' in results and not results['merged'].empty:
        results['merged'].to_csv(out, index=False)
        out.seek(0)
        results['merged_csv_bytes'] = out.read()
    # Create a tiny Excel report too
    out2 = BytesIO()
    with pd.ExcelWriter(out2, engine='openpyxl') as writer:
        if 'merged' in results:
            results['merged'].to_excel(writer, sheet_name='merged', index=False)
        if 'numeric_stats' in results:
            results['numeric_stats'].to_excel(writer, sheet_name='numeric_stats')
        if 'summary' in results:
            results['summary'].to_excel(writer, sheet_name='summary', index=False)
    out2.seek(0)
    results['final_report_xlsx'] = out2.read()

    return results

# === END: PLACEHOLDER FOR ORIGINAL CODE 2 ===

# -------------------------
# Streamlit UI
# -------------------------

st.title('Combined Processor: CSV → Excel → Analysis')
st.markdown(
    """
    Upload your raw CSV file(s) for Stage 1 (these may be multiple .csv). The app will run the Stage 1 processing
    and produce an Excel workbook which you can download. You can then feed that Excel workbook into Stage 2.

    If you already have your original Code 1 and Code 2 functions, paste them into the placeholders in this file
    (`process_raw_csvs_to_excel` and `process_excel_stage2`) so none of your original logic is lost.
    """
)

st.sidebar.header('Workflow')
step = st.sidebar.radio('Choose step', ['Stage 1: CSV -> Excel', 'Stage 2: Excel -> Report', 'About & GitHub'])

if step == 'Stage 1: CSV -> Excel':
    st.header('Stage 1 — Upload raw CSV file(s)')
    uploaded_files = st.file_uploader('Upload one or more CSV files', accept_multiple_files=True, type=['csv'])
    if uploaded_files:
        dfs = {}
        st.write(f'{len(uploaded_files)} file(s) uploaded')
        for f in uploaded_files:
            try:
                df = safe_read_csv(f)
                dfs[f.name] = df
                st.write(f'**{f.name}** — {df.shape[0]} rows × {df.shape[1]} cols')
                st.dataframe(df.head(3))
            except Exception as e:
                st.error(f'Failed to read {f.name}: {e}')

        st.markdown('---')
        if st.button('Run Stage 1 processing and generate Excel'):
            with st.spinner('Processing...'):
                excel_bytes, meta = process_raw_csvs_to_excel(dfs)
                # save to session state for Stage 2 usage
                st.session_state['stage1_excel'] = excel_bytes
                st.success('Stage 1 complete. Excel workbook generated and available to download.')
                # provide download
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                fn = f'stage1_output_{timestamp}.xlsx'
                st.download_button('Download Stage 1 Excel', data=excel_bytes, file_name=fn, mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
                st.write('Summary:')
                if 'summary' in meta:
                    st.dataframe(meta['summary'])
    else:
        st.info('Upload one or more CSV files to get started.')

elif step == 'Stage 2: Excel -> Report':
    st.header('Stage 2 — Upload Excel workbook (from Stage 1)')
    excel_file = st.file_uploader('Upload Excel (.xlsx) generated by Stage 1', type=['xlsx'])

    # allow using the session-state stage1 Excel if available
    if 'stage1_excel' in st.session_state and excel_file is None:
        use_session = st.checkbox('Use the Excel generated in this session (Stage 1)', value=True)
        if use_session:
            excel_bytes = st.session_state['stage1_excel']
        else:
            excel_bytes = None
    else:
        excel_bytes = excel_file.read() if excel_file else None

    if excel_bytes:
        st.write('Excel file ready for Stage 2 processing')
        if st.button('Run Stage 2 processing and generate final report'):
            with st.spinner('Running Stage 2...'):
                try:
                    results = process_excel_stage2(excel_bytes)
                    st.success('Stage 2 complete.')
                    # show small previews
                    if 'summary' in results:
                        st.subheader('Summary (from Stage 1)')
                        st.dataframe(results['summary'])
                    if 'numeric_stats' in results:
                        st.subheader('Numeric column stats')
                        st.dataframe(results['numeric_stats'])
                    # provide downloads
                    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
                    if 'final_report_xlsx' in results:
                        st.download_button('Download final Excel report', data=results['final_report_xlsx'], file_name=f'final_report_{ts}.xlsx', mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
                    if 'merged_csv_bytes' in results:
                        st.download_button('Download merged CSV (stage2)', data=results['merged_csv_bytes'], file_name=f'merged_{ts}.csv', mime='text/csv')
                except Exception as e:
                    st.error(f'Stage 2 failed: {e}')
    else:
        st.info('Upload an Excel (.xlsx) file from Stage 1 or run Stage 1 in this session.')

else:
    st.header('About & GitHub')
    st.markdown(
        """
        This app is a combined frontend for two separate processing programs (Code 1 and Code 2).

        How to use on GitHub:

        1. Create a new GitHub repository (public or private).
        2. Add these files to the repository root:
           - app.py (this file)
           - requirements.txt (see below)
        3. (Optional) add a .gitignore and a README.md describing your app.
        4. Deploy to Streamlit Cloud or run locally with `streamlit run app.py`.

        If you want me to insert your exact Code 1 and Code 2 logic into the placeholders, upload the two .py files or paste them here and I'll merge them into this template without dropping any details.
        """
    )
    st.subheader('requirements.txt')
    st.code('''
    streamlit>=1.20.0
    pandas
    openpyxl
    ''')

# -------------------------
# End
# -------------------------
